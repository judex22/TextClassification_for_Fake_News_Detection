{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/judex22/TextClassification_for_Fake_News_Detection/blob/main/Fake_News_Detection_Trigram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqUqJBEVk8AW"
      },
      "source": [
        "# NLP Assignment 1 (40% of grade): Text classification for Fake News Detection (Trigram BOW)\n",
        "\n",
        "This coursework will involve you implementing functions for a text classifier, which you will train to detect **fake news** in a corpus of approx. 10,000 statements, which will be split into a 80%/20% training/test split.\n",
        "\n",
        "In this template you are given the basis for that implementation, though some of the functions are missing, which you have to fill in.\n",
        "\n",
        "Follow the instructions file **NLP_Assignment_1_Instructions.pdf** for details of each question - the outline of what needs to be achieved for each question is as below.\n",
        "\n",
        "You must submit all **ipython notebooks and extra resources you need to run the code if you've added them** in the code submission, and a **2 page report (pdf)** in the report submission on QMPlus where you report your methods and findings according to the instructions file for each question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwtpLlqhk8AZ",
        "outputId": "31f3b384-3a04-49bc-81c8-434165532909"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\jude sequeira\\anaconda3\\lib\\site-packages (3.7)\n",
            "Requirement already satisfied: click in c:\\users\\jude sequeira\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: joblib in c:\\users\\jude sequeira\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jude sequeira\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
            "Requirement already satisfied: tqdm in c:\\users\\jude sequeira\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\jude sequeira\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_P3Mquvk8Aa",
        "outputId": "0115f30f-e452-478a-f838-9a4009fe2572"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\jude\n",
            "[nltk_data]     sequeira\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to C:\\Users\\jude\n",
            "[nltk_data]     sequeira\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to C:\\Users\\jude\n",
            "[nltk_data]     sequeira\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to C:\\Users\\jude\n",
            "[nltk_data]     sequeira\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import csv                               # csv reader\n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk.classify import SklearnClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import precision_recall_fscore_support # to report on precision and recall\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import accuracy_score\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P40lhsvk8Aa"
      },
      "outputs": [],
      "source": [
        "def load_data(path):\n",
        "    \"\"\"Load data from a tab-separated file and append it to raw_data.\"\"\"\n",
        "    with open(path,encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter='\\t')\n",
        "        for line in reader:\n",
        "            if line[0] == \"Id\":  # skip header\n",
        "                continue\n",
        "            (label, text) = parse_data_line(line)\n",
        "            raw_data.append((text, label))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_and_preprocess_data(percentage):\n",
        "    \"\"\"Split the data between train_data and test_data according to the percentage\n",
        "    and performs the preprocessing.\"\"\"\n",
        "    num_samples = len(raw_data)\n",
        "    num_training_samples = int((percentage * num_samples))\n",
        "    for (text, label) in raw_data[:num_training_samples]:\n",
        "        train_data.append((to_feature_vector(pre_process(text)),label))\n",
        "\n",
        "    for (text, label) in raw_data[num_training_samples:]:\n",
        "        test_data.append((to_feature_vector(pre_process(text)),label))\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4avMrpFpk8Aa"
      },
      "source": [
        "# Question 1: Input and Basic preprocessing (10 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFrq61EAk8Aa"
      },
      "outputs": [],
      "source": [
        "def convert_label(label):\n",
        "    \"\"\"Converts the multiple classes into two,\n",
        "    making it a binary distinction between fake news and real.\"\"\"\n",
        "    #return label\n",
        "    # Converting the multiclass labels to binary label\n",
        "    labels_map = {\n",
        "        'true': 'REAL',\n",
        "        'mostly-true': 'REAL',\n",
        "        'half-true': 'REAL',\n",
        "        'false': 'FAKE',\n",
        "        'barely-true': 'FAKE',\n",
        "        'pants-fire': 'FAKE'\n",
        "    }\n",
        "    return labels_map[label]\n",
        "\n",
        "\n",
        "def parse_data_line(data_line):\n",
        "\n",
        "    # Should return a tuple of the label as just FAKE or REAL and the statement\n",
        "    # e.g. (label, statement)\n",
        "    l=data_line[1]\n",
        "\n",
        "\n",
        "    if (l==\"true\")or(l==\"mostly-true\")or(l==\"half-true\")or(l==\"false\")or(l==\"barely-true\")or(l==\"pants-fire\"):\n",
        "        Label=convert_label(l)\n",
        "        sent=data_line[2]\n",
        "\n",
        "\n",
        "    return (Label,sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV_C6dj6k8Ab"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import regex as re\n",
        "\n",
        "\n",
        "# Input: a string of one statement\n",
        "def pre_process(text):\n",
        "    # Should return a list of tokens\n",
        "    # DESCRIBE YOUR METHOD IN WORDS\n",
        "\n",
        "    text=text.translate(str.maketrans('','',string.punctuation))\n",
        "    tokens = re.split(r\"\\s+\",text)\n",
        "    tokens= [x.lower() for x in tokens]\n",
        "    stop_words=set(stopwords.words(\"english\"))\n",
        "    tokens=[i for i in tokens if i not in stop_words]\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    tokens=[lemmatizer.lemmatize(w,pos='v') for w in tokens]\n",
        "    if False:\n",
        "\n",
        "        print(\"Output after PreProcessing Tokens:\\n\",tokens)\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYURoiTbk8Ab"
      },
      "source": [
        "# Question 2: Basic Feature Extraction (20 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyZ7Ucsik8Ab"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "global_feature_dict = {} # A global dictionary of features\n",
        "\n",
        "def to_feature_vector(tokens):\n",
        "\n",
        "\n",
        "\n",
        "    # Should return a dictionary containing features as keys, and weights as values\n",
        "    # DESCRIBE YOUR METHOD IN WORDS\n",
        "\n",
        "    featureDict = {}  # Global feature dictionary maps from words/features to unique index\n",
        "    i = 0 # index counter variable for the global feature dict\n",
        "\n",
        "\n",
        "    \"\"\"Convert word tokens into a feature vector dictionary of\n",
        "    feature index keys and weight values.\n",
        "\n",
        "    Update the featureVector if in training phase, i.e. where training=True.\n",
        "    \"\"\"\n",
        "\n",
        "    featureVector = {}                       # local feature vector for this example (in dict form)\n",
        "\n",
        "    def generate_N_grams(text,ngram):                #function to get N-gram tokens(in this case N=3 i:e Trigram)\n",
        "\n",
        "        words=text\n",
        "        temp=zip(*[words[i:] for i in range(0,ngram)])\n",
        "        ans=[' '.join(ngram) for ngram in temp]\n",
        "\n",
        "        return ans\n",
        "\n",
        "    trigram_token=generate_N_grams(tokens,3)\n",
        "\n",
        "\n",
        "    for w in trigram_token:\n",
        "\n",
        "        # First add to the global feature dictionary, and give it a position\n",
        "        # if in training\n",
        "        try:\n",
        "            # if the word w is in the dictionary,\n",
        "            # then assign i to its value in the dictionary\n",
        "            i = featureDict[w]\n",
        "        except KeyError:\n",
        "            # else if it's not in the dictionary,\n",
        "            # it's a new feature in training, add new entry\n",
        "            i = len(featureDict) + 1\n",
        "            featureDict[w] = i\n",
        "\n",
        "\n",
        "        # Add to local feature vector (as a dictionary entry with a weight)\n",
        "\n",
        "        try:\n",
        "            # if the word w is in the local feature vector (repeat word), add 1/n weight\n",
        "            featureVector[i] += 1 #(1.0/len(words))\n",
        "        except KeyError:\n",
        "            # else if it's not in the local feature vector, create the entry with 1/n weight\n",
        "            featureVector[i] = 1 #(1.0/len(words))\n",
        "\n",
        "        if False:\n",
        "            # example binary alternative to the above (just add it as 1 if present)\n",
        "            if not i in featureVector.keys():\n",
        "                featureVector[i] = 1\n",
        "    print(\"Feature Dictionary of Trigram BOW model\\n\",featureDict)\n",
        "    print(\"Feature Vector of Trigram BOW model\\n\",featureVector,\"\\n\")\n",
        "\n",
        "    return featureVector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMXPvPmak8Ab"
      },
      "outputs": [],
      "source": [
        "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
        "\n",
        "def train_classifier(data):\n",
        "    print(\"Training Classifier...\")\n",
        "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
        "    return SklearnClassifier(pipeline).train(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DaC8l0qk8Ab"
      },
      "source": [
        "# Question 3: Cross-validation (20 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75oPyKvlk8Ac"
      },
      "outputs": [],
      "source": [
        "#solution\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from statistics import mean\n",
        "\n",
        "\n",
        "def cross_validate(dataset, folds):\n",
        "\n",
        "    train=dataset\n",
        "    results =[]\n",
        "    cv_results=[]\n",
        "    cv=[]\n",
        "    prec=[]\n",
        "    rec=[]\n",
        "    f1=[]\n",
        "\n",
        "\n",
        "\n",
        "    fold_size = int(len(dataset)/folds) + 1\n",
        "\n",
        "    for i in range(0,len(dataset),int(fold_size)):\n",
        "        # insert code here that trains and tests on the 10 folds of data in the dataset\n",
        "\n",
        "\n",
        "        k_test= train[i:fold_size+i]\n",
        "        k_train= train[0:i]+train[fold_size+i: ]\n",
        "\n",
        "        k_train_label=[]\n",
        "        k_test_label=[]\n",
        "\n",
        "\n",
        "        for j in range(0,len(k_test)):\n",
        "            k_test_label.append(k_test[j][1])\n",
        "\n",
        "        print(\"Fold start on items %d - %d\" % (i, i+fold_size))\n",
        "        # FILL IN THE METHOD HERE\n",
        "\n",
        "        classifier = train_classifier(k_train)\n",
        "        k_test_true = k_test_label                   # get the ground-truth labels from the data\n",
        "        k_test_pred = predict_labels([x[0] for x in k_test], classifier)\n",
        "\n",
        "        results = precision_recall_fscore_support(k_test_true, k_test_pred ,average= 'weighted' )\n",
        "        cv.append(results)\n",
        "\n",
        "        if True:\n",
        "            print(\"Precision: %f\\nRecall: %f\\nF Score:%f \\n\" % results[:3])\n",
        "            print(\"Classification Report on 10 fold CV Test Data:\\n\",classification_report(k_test_true, k_test_pred) ,\"\\n\")\n",
        "\n",
        "\n",
        "    for q in range(0,10):\n",
        "        prec.append(cv[q][0])\n",
        "        rec.append(cv[q][1])\n",
        "        f1.append(cv[q][2])\n",
        "\n",
        "    prec_avg=mean(prec)\n",
        "    rec_avg=mean(rec)\n",
        "    f1_avg=mean(f1)\n",
        "\n",
        "\n",
        "    cv_results=[prec_avg,rec_avg,f1_avg]\n",
        "\n",
        "    print(\"\\n\\n The Average of Precision Recall F1 score across all 10 folds of Cross Validation:\")\n",
        "    return cv_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a40JT-2Kk8Ac"
      },
      "outputs": [],
      "source": [
        "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
        "\n",
        "def predict_labels(samples, classifier):\n",
        "    \"\"\"Assuming preprocessed samples, return their predicted labels from the classifier model.\"\"\"\n",
        "    return classifier.classify_many(samples)\n",
        "\n",
        "def predict_label_from_raw(sample, classifier):\n",
        "    \"\"\"Assuming raw text, return its predicted label from the classifier model.\"\"\"\n",
        "    return classifier.classify(to_feature_vector(preProcess(reviewSample)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3QA4k7qk8Ac"
      },
      "outputs": [],
      "source": [
        "# MAIN\n",
        "\n",
        "# loading reviews\n",
        "# initialize global lists that will be appended to by the methods below\n",
        "raw_data = []          # the filtered data from the dataset file\n",
        "train_data = []        # the pre-processed training data as a percentage of the total dataset\n",
        "test_data = []         # the pre-processed test data as a percentage of the total dataset\n",
        "\n",
        "\n",
        "# references to the data files\n",
        "data_file_path = 'fake_news.tsv'\n",
        "\n",
        "# Do the actual stuff (i.e. call the functions we've made)\n",
        "# We parse the dataset and put it in a raw data list\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
        "      \"Preparing the dataset...\",sep='\\n')\n",
        "\n",
        "load_data(data_file_path)\n",
        "\n",
        "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
        "# You do the cross validation on the 80% (training data)\n",
        "# We print the number of training samples and the number of features before the split\n",
        "print(\"Now %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
        "      \"Preparing training and test data...\",sep='\\n')\n",
        "\n",
        "split_and_preprocess_data(0.8)\n",
        "\n",
        "# We print the number of training samples and the number of features after the split\n",
        "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(raw_data), len(train_data), len(test_data)),\n",
        "      \"Training Samples: \", len(train_data), \"Features: \", len(global_feature_dict), sep='\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMtBBYq1k8Ac",
        "outputId": "c9de600b-c074-4af7-de45-5d497abce7d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold start on items 0 - 820\n",
            "Training Classifier...\n",
            "Precision: 0.551800\n",
            "Recall: 0.586585\n",
            "F Score:0.450469 \n",
            "\n",
            "Classification Report on 10 fold CV Test Data:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        FAKE       0.50      0.02      0.05       339\n",
            "        REAL       0.59      0.98      0.74       481\n",
            "\n",
            "    accuracy                           0.59       820\n",
            "   macro avg       0.54      0.50      0.39       820\n",
            "weighted avg       0.55      0.59      0.45       820\n",
            " \n",
            "\n",
            "Fold start on items 820 - 1640\n",
            "Training Classifier...\n",
            "Precision: 0.525058\n",
            "Recall: 0.542683\n",
            "F Score:0.438400 \n",
            "\n",
            "Classification Report on 10 fold CV Test Data:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        FAKE       0.50      0.08      0.14       375\n",
            "        REAL       0.55      0.93      0.69       445\n",
            "\n",
            "    accuracy                           0.54       820\n",
            "   macro avg       0.52      0.51      0.42       820\n",
            "weighted avg       0.53      0.54      0.44       820\n",
            " \n",
            "\n",
            "Fold start on items 1640 - 2460\n",
            "Training Classifier...\n",
            "Precision: 0.534157\n",
            "Recall: 0.524390\n",
            "F Score:0.383628 \n",
            "\n",
            "Classification Report on 10 fold CV Test Data:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        FAKE       0.55      0.03      0.06       392\n",
            "        REAL       0.52      0.98      0.68       428\n",
            "\n",
            "    accuracy                           0.52       820\n",
            "   macro avg       0.53      0.50      0.37       820\n",
            "weighted avg       0.53      0.52      0.38       820\n",
            " \n",
            "\n",
            "Fold start on items 2460 - 3280\n",
            "Training Classifier...\n",
            "Precision: 0.575294\n",
            "Recall: 0.569512\n",
            "F Score:0.438317 \n",
            "\n",
            "Classification Report on 10 fold CV Test Data:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        FAKE       0.58      0.04      0.07       357\n",
            "        REAL       0.57      0.98      0.72       463\n",
            "\n",
            "    accuracy                           0.57       820\n",
            "   macro avg       0.58      0.51      0.40       820\n",
            "weighted avg       0.58      0.57      0.44       820\n",
            " \n",
            "\n",
            "Fold start on items 3280 - 4100\n",
            "Training Classifier...\n",
            "Precision: 0.520338\n",
            "Recall: 0.571951\n",
            "F Score:0.443056 \n",
            "\n",
            "Classification Report on 10 fold CV Test Data:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        FAKE       0.44      0.03      0.06       348\n",
            "        REAL       0.58      0.97      0.72       472\n",
            "\n",
            "    accuracy                           0.57       820\n",
            "   macro avg       0.51      0.50      0.39       820\n",
            "weighted avg       0.52      0.57      0.44       820\n",
            " \n",
            "\n",
            "Fold start on items 4100 - 4920\n",
            "Training Classifier...\n",
            "Precision: 0.585403\n",
            "Recall: 0.580488\n",
            "F Score:0.454291 \n",
            "\n",
            "Classification Report on 10 fold CV Test Data:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        FAKE       0.59      0.05      0.09       349\n",
            "        REAL       0.58      0.98      0.73       471\n",
            "\n",
            "    accuracy                           0.58       820\n",
            "   macro avg       0.59      0.51      0.41       820\n",
            "weighted avg       0.59      0.58      0.45       820\n",
            " \n",
            "\n",
            "Fold start on items 4920 - 5740\n",
            "Training Classifier...\n",
            "Precision: 0.550160\n",
            "Recall: 0.575610\n",
            "F Score:0.466919 \n",
            "\n",
            "Classification Report on 10 fold CV Test Data:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        FAKE       0.51      0.07      0.13       349\n",
            "        REAL       0.58      0.95      0.72       471\n",
            "\n",
            "    accuracy                           0.58       820\n",
            "   macro avg       0.54      0.51      0.42       820\n",
            "weighted avg       0.55      0.58      0.47       820\n",
            " \n",
            "\n",
            "Fold start on items 5740 - 6560\n",
            "Training Classifier...\n",
            "Precision: 0.561422\n",
            "Recall: 0.568293\n",
            "F Score:0.441332 \n",
            "\n",
            "Classification Report on 10 fold CV Test Data:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        FAKE       0.55      0.04      0.08       357\n",
            "        REAL       0.57      0.97      0.72       463\n",
            "\n",
            "    accuracy                           0.57       820\n",
            "   macro avg       0.56      0.51      0.40       820\n",
            "weighted avg       0.56      0.57      0.44       820\n",
            " \n",
            "\n",
            "Fold start on items 6560 - 7380\n",
            "Training Classifier...\n",
            "Precision: 0.475738\n",
            "Recall: 0.543902\n",
            "F Score:0.425022 \n",
            "\n",
            "Classification Report on 10 fold CV Test Data:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        FAKE       0.38      0.05      0.08       363\n",
            "        REAL       0.55      0.94      0.70       457\n",
            "\n",
            "    accuracy                           0.54       820\n",
            "   macro avg       0.47      0.49      0.39       820\n",
            "weighted avg       0.48      0.54      0.43       820\n",
            " \n",
            "\n",
            "Fold start on items 7380 - 8200\n",
            "Training Classifier...\n",
            "Precision: 0.498203\n",
            "Recall: 0.572660\n",
            "F Score:0.466811 \n",
            "\n",
            "Classification Report on 10 fold CV Test Data:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        FAKE       0.37      0.06      0.10       333\n",
            "        REAL       0.59      0.93      0.72       479\n",
            "\n",
            "    accuracy                           0.57       812\n",
            "   macro avg       0.48      0.49      0.41       812\n",
            "weighted avg       0.50      0.57      0.47       812\n",
            " \n",
            "\n",
            "\n",
            "\n",
            " The Average of Precision Recall F1 score across all 10 folds of Cross Validation:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.5377572555125357, 0.5636074732668509, 0.4408244305497541]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cross_validate(train_data, 10)  # will work and output overall performance of p, r, f-score when cv implemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4MKu1m8k8Ac"
      },
      "source": [
        "# 4. Error Analysis (10 marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Qj9pz1ok8Ac"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "# a function to make the confusion matrix readable and pretty\n",
        "def confusion_matrix_heatmap(y_test, preds, labels):\n",
        "    \"\"\"Function to plot a confusion matrix\"\"\"\n",
        "    # pass labels to the confusion matrix function to ensure right order\n",
        "    cm = metrics.confusion_matrix(y_test, preds, labels=labels)\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(cm)\n",
        "    plt.title('Confusion matrix of the classifier')\n",
        "    fig.colorbar(cax)\n",
        "    ax.set_xticks(np.arange(len(labels)))\n",
        "    ax.set_yticks(np.arange(len(labels)))\n",
        "    ax.set_xticklabels( labels, rotation=45)\n",
        "    ax.set_yticklabels( labels)\n",
        "\n",
        "    for i in range(len(cm)):\n",
        "        for j in range(len(cm)):\n",
        "            text = ax.text(j, i, cm[i, j],\n",
        "                           ha=\"center\", va=\"center\", color=\"w\")\n",
        "\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    # fix for mpl bug that cuts off top/bottom of seaborn viz:\n",
        "    b, t = plt.ylim() # discover the values for bottom and top\n",
        "    b += 0.5 # Add 0.5 to the bottom\n",
        "    t -= 0.5 # Subtract 0.5 from the top\n",
        "    plt.ylim(b, t) # update the ylim(bottom, top) values\n",
        "    plt.show() # ta-da!\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzMJ6FKNk8Ad",
        "outputId": "e9b4b895-240c-4b8e-d309-5e1f153c66fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold start on items 0 - 820\n",
            "Training Classifier...\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of K-Fold Validation Data 0.5865853658536585\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "import statistics\n",
        "r =[]\n",
        "\n",
        "folds=10\n",
        "fold_size = int(len(train_data)/folds) + 1\n",
        "i=0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# confusion Matrix for first fold of 10 fold cross validation\n",
        "\n",
        "k_test1= train_data[i:fold_size+i]\n",
        "k_train1= train_data[0:i]+train_data[fold_size+i: ]\n",
        "\n",
        "k_train_label1=[]\n",
        "k_test_label1=[]\n",
        "\n",
        "\n",
        "for j in range(0,len(k_test1)):\n",
        "    k_test_label1.append(k_test1[j][1])\n",
        "\n",
        "print(\"Fold start on items %d - %d\" % (i, i+fold_size))\n",
        "        # FILL IN THE METHOD HERE\n",
        "\n",
        "classifier1 = train_classifier(k_train1)\n",
        "k_test_true1 = k_test_label1                   # get the ground-truth labels from the data\n",
        "k_test_pred1 = predict_labels([x[0] for x in k_test1], classifier1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "labels=['REAL', 'FAKE']\n",
        "confusion_matrix_heatmap(k_test_true1,k_test_pred1,labels)\n",
        "print(\"Accuracy of K-Fold Validation Data\",accuracy_score(k_test_true1,k_test_pred1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w4tpgWsk8Ad"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}